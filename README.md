# DPO-ST: DPO-augmented Self-Training

This repository contains the official code and data for the paper **Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning**


## Citation
If you find this paper useful, please consider citing it
```
@inproceedings{wang2024dpost,
      title={Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning}, 
      author={Tianduo Wang and Shichen Li and Wei Lu},
      year={2024},
      booktitle = {Proceedings of ACL},
}
```

## Acknowledgement
This repo is largely inspired by [GSM8K-ScRel](https://github.com/OFA-Sys/gsm8k-ScRel) and [TRL](https://github.com/huggingface/trl). We are grateful to the authors for their brilliant work.